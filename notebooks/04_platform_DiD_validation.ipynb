{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1352affb",
   "metadata": {},
   "source": [
    "## Notebook Purpose:\n",
    "\n",
    "1. The primary goal of this notebook is to validate the results produced by the platform.\n",
    "2. We are picking one experiment = `exp_big_exp_run` and we are going to validate following:\n",
    "    - DiD Results\n",
    "3. This validation will ensure if the platform is trustworthy or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4d75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 04:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Laptop_ROG_CUPED_validation\")\n",
    "        .master(\"spark://10.0.0.80:7077\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.executor.cores\", \"10\")\n",
    "        .config(\"spark.executor.memory\", \"18g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "\n",
    "        .config(\"spark.driver.memory\", \"10g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "        .config(\"spark.driver.host\", \"10.0.0.80\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "        # AQE + shuffle\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"288\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"256m\")\n",
    "\n",
    "        # Don’t set spark.local.dir here; use SPARK_LOCAL_DIRS on the worker\n",
    "        # .config(\"spark.local.dir\", \"...\")  <-- remove\n",
    "            # -------- MinIO / S3A (must match Iceberg) --------\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://10.0.0.80:9100\")  # <-- changed\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "        # -------- Iceberg REST catalog --------\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "        # .config(\"spark.sql.catalog.iceberg.uri\", \"http://10.0.0.59:8181\")\n",
    "        .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg-warehouse/warehouse/\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"iceberg\")\n",
    "        .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "        # Iceberg's own S3 settings (again, pointing to MinIO)\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://10.0.0.80:9100\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"minioadmin\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"minioadmin\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            \"/opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.6.0.jar,\"\n",
    "            \"/opt/spark/jars/iceberg-aws-bundle-1.6.0.jar\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4946688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "create or replace temporary view did_results as\n",
    "\n",
    "with t1 as(\n",
    "select distinct experiment_id, user_id, variant\n",
    "from iceberg.exp.exposures\n",
    "where experiment_id = 'exp_big_exp_run'\n",
    "),\n",
    "\n",
    "t2 as (\n",
    "select experiment_id, user_id, metric_name, value\n",
    "from iceberg.exp.outcomes\n",
    "where metric_name in ('revenue','pre_revenue')\n",
    "and experiment_id = 'exp_big_exp_run'\n",
    ")\n",
    "\n",
    "select t1.experiment_id, t2.user_id, t1.variant,\n",
    "max(case when metric_name = 'revenue' then value else 0 end) as post,\n",
    "max(case when metric_name = 'pre_revenue' then value else 0 end) as pre\n",
    "\n",
    "from t1\n",
    "inner join t2\n",
    "on t1.experiment_id =  t2.experiment_id\n",
    "and t1.user_id =  t2.user_id\n",
    "\n",
    "group by  t1.experiment_id, t2.user_id, t1.variant\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fadb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|  experiment_id|metric_name|\n",
      "+---------------+-----------+\n",
      "|exp_big_exp_run|pre_revenue|\n",
      "|exp_big_exp_run| conversion|\n",
      "|exp_big_exp_run|    revenue|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select distinct experiment_id,metric_name from iceberg.exp.outcomes\n",
    "where experiment_id = 'exp_big_exp_run'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cfa00f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 04:27:42 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+------------------+------------------+\n",
      "|  experiment_id|  user_id|  variant|              post|               pre|\n",
      "+---------------+---------+---------+------------------+------------------+\n",
      "|exp_big_exp_run|u_0000028|  control| 4.660211329628044| 7.942785605843696|\n",
      "|exp_big_exp_run|u_0000044|  control|15.045331295098066| 17.61707980444741|\n",
      "|exp_big_exp_run|u_0000045|  control|10.761676001676506|14.615228707203336|\n",
      "|exp_big_exp_run|u_0000046|treatment|12.961100944601199|12.344331046091192|\n",
      "|exp_big_exp_run|u_0000059|  control|  7.10428510891265|15.075387570944553|\n",
      "|exp_big_exp_run|u_0000062|treatment| 9.219429144458251|16.087307296706186|\n",
      "|exp_big_exp_run|u_0000079|treatment|12.455239950701042| 12.94523091436157|\n",
      "|exp_big_exp_run|u_0000082|treatment| 9.623341036448572|  9.81912435744377|\n",
      "|exp_big_exp_run|u_0000083|treatment|13.554762100421534|13.503712096797475|\n",
      "|exp_big_exp_run|u_0000084|  control|14.657482410076426|15.376523068107865|\n",
      "|exp_big_exp_run|u_0000093|  control| 9.624065226857883|  7.28044486968413|\n",
      "|exp_big_exp_run|u_0000094|  control| 8.276345667563394|14.634285265203363|\n",
      "|exp_big_exp_run|u_0000132|treatment|15.289864789657898|14.648452902943177|\n",
      "|exp_big_exp_run|u_0000141|  control| 8.335739235061032|11.621513317101558|\n",
      "|exp_big_exp_run|u_0000144|treatment|10.530071363830267|12.555179036844505|\n",
      "|exp_big_exp_run|u_0000147|  control|14.400557965076445|10.944257319037206|\n",
      "|exp_big_exp_run|u_0000149|treatment|14.614107500990544|14.563527039229967|\n",
      "|exp_big_exp_run|u_0000154|  control|15.574692846481007|12.072501219387169|\n",
      "|exp_big_exp_run|u_0000159|treatment| 15.58393256441715|10.093488391386634|\n",
      "|exp_big_exp_run|u_0000166|treatment|11.742759098796618| 16.74820832869505|\n",
      "+---------------+---------+---------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select * from did_results \n",
    "where post <> 0\n",
    "limit 20\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba8a86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in Differences Estimate:  0.6196836438164492\n"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select variant , avg(pre) pre_mean,\n",
    "avg(post) as post_mean\n",
    "from did_results\n",
    "group by variant\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# pre_post_means = spark.sql(v_sql).toPandas()\n",
    "pre_post_means\n",
    "did = (pre_post_means[pre_post_means['variant']=='treatment']['post_mean'].values[0] - pre_post_means[pre_post_means['variant']=='treatment']['pre_mean'].values[0] \n",
    "          - pre_post_means[pre_post_means['variant']=='control']['post_mean'].values[0] + pre_post_means[pre_post_means['variant']=='control']['pre_mean'].values[0])\n",
    "print(f\"Difference in Differences Estimate:  {did}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a8537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 04:34:02 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "26/02/19 04:34:02 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>period</th>\n",
       "      <th>measure</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.877119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.252520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.504177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.499261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group  period    measure  interaction\n",
       "0      1       1   1.877119            1\n",
       "1      0       1   1.252520            0\n",
       "2      1       0  12.504177            0\n",
       "3      0       0  12.499261            0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "with t1 as (select case when variant = 'treatment' then 1 else 0 end as group, 1 as period,\n",
    "avg(post) as measure\n",
    "from did_results\n",
    "group by case when variant = 'treatment' then 1 else 0 end\n",
    "union all\n",
    "select case when variant = 'treatment' then 1 else 0 end as group, 0 as period,\n",
    "avg(pre) as measure\n",
    "from did_results\n",
    "group by case when variant = 'treatment' then 1 else 0 end)\n",
    "\n",
    "select t1.*, group*period as interaction\n",
    "from t1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "matrix = spark.sql(v_sql).toPandas()\n",
    "\n",
    "matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3410a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 04:44:26 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+----+------------------+\n",
      "|  experiment_id|  user_id|  variant|post|               pre|\n",
      "+---------------+---------+---------+----+------------------+\n",
      "|exp_big_exp_run|u_0000001|treatment| 0.0|16.311533793776313|\n",
      "|exp_big_exp_run|u_0000002|treatment| 0.0| 11.32790644644474|\n",
      "|exp_big_exp_run|u_0000003|  control| 0.0|14.174728490227805|\n",
      "|exp_big_exp_run|u_0000004|treatment| 0.0|10.847600353402715|\n",
      "|exp_big_exp_run|u_0000005|  control| 0.0| 8.275909435378757|\n",
      "|exp_big_exp_run|u_0000006|  control| 0.0|12.443259526513518|\n",
      "|exp_big_exp_run|u_0000007|treatment| 0.0|15.835280615716401|\n",
      "|exp_big_exp_run|u_0000008|treatment| 0.0| 7.090048635555965|\n",
      "|exp_big_exp_run|u_0000010|  control| 0.0|  9.99508481757065|\n",
      "|exp_big_exp_run|u_0000011|treatment| 0.0|16.328538904577922|\n",
      "+---------------+---------+---------+----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from did_results limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e4f88bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 04:49:11 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "26/02/19 04:49:11 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select user_id, case when variant = 'treatment' then 1 else 0 end as group,\n",
    "1 as period, post as measure\n",
    "from did_results \n",
    "union all\n",
    "select user_id, case when variant = 'treatment' then 1 else 0 end as group,\n",
    "0 as period, pre as measure\n",
    "from did_results \n",
    "\n",
    "\"\"\"\n",
    "raw_df = spark.sql(v_sql).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dcef357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    1699422\n",
       "group      1699422\n",
       "period     1699422\n",
       "measure    1699422\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d6e0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                measure   R-squared:                       0.688\n",
      "Model:                            OLS   Adj. R-squared:                  0.688\n",
      "Method:                 Least Squares   F-statistic:                 1.288e+06\n",
      "Date:                Thu, 19 Feb 2026   Prob (F-statistic):               0.00\n",
      "Time:                        04:49:53   Log-Likelihood:            -4.6277e+06\n",
      "No. Observations:             1699422   AIC:                         9.255e+06\n",
      "Df Residuals:                 1699418   BIC:                         9.256e+06\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:              cluster                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept       12.4993      0.005   2719.518      0.000      12.490      12.508\n",
      "group            0.0049      0.007      0.756      0.450      -0.008       0.018\n",
      "period         -11.2467      0.008  -1496.408      0.000     -11.261     -11.232\n",
      "group:period     0.6197      0.011     54.833      0.000       0.598       0.642\n",
      "==============================================================================\n",
      "Omnibus:                   664246.152   Durbin-Watson:                   2.005\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2663178.806\n",
      "Skew:                           1.949   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.735   Cond. No.                         6.86\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(\n",
    "    \"measure ~ group + period + group:period\",\n",
    "    data=raw_df\n",
    ").fit(cov_type=\"cluster\", cov_kwds={\"groups\": raw_df[\"user_id\"]})\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540799d",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "As shown below when we compare manually calculated values with platform values there are exactly the same and this further shows that platform results can be trusted.\n",
    "\n",
    "**Manual Calculation:**\n",
    "\n",
    "` Difference in Differences Estimate:  0.6196836438164492 `\n",
    "\n",
    "``` \n",
    "OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                measure   R-squared:                       0.688\n",
    "Model:                            OLS   Adj. R-squared:                  0.688\n",
    "Method:                 Least Squares   F-statistic:                 1.288e+06\n",
    "Date:                Thu, 19 Feb 2026   Prob (F-statistic):               0.00\n",
    "Time:                        04:49:53   Log-Likelihood:            -4.6277e+06\n",
    "No. Observations:             1699422   AIC:                         9.255e+06\n",
    "Df Residuals:                 1699418   BIC:                         9.256e+06\n",
    "Df Model:                           3                                         \n",
    "Covariance Type:              cluster                                         \n",
    "================================================================================\n",
    "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "Intercept       12.4993      0.005   2719.518      0.000      12.490      12.508\n",
    "group            0.0049      0.007      0.756      0.450      -0.008       0.018\n",
    "period         -11.2467      0.008  -1496.408      0.000     -11.261     -11.232\n",
    "group:period     0.6197      0.011     54.833      0.000       0.598       0.642\n",
    "==============================================================================\n",
    "Omnibus:                   664246.152   Durbin-Watson:                   2.005\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2663178.806\n",
    "Skew:                           1.949   Prob(JB):                         0.00\n",
    "Kurtosis:                       7.735   Cond. No.                         6.86\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors are robust to cluster correlation (cluster)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Platform results:**\n",
    "\n",
    "\n",
    "metric_pre: pre_revenue metric_post: revenue metric_name (stored): revenue\n",
    "\n",
    "window: 2026-01-01 00:00:00 → 2026-01-29 00:00:00\n",
    "\n",
    "Cell means\n",
    "- control_pre: 12.4993\n",
    "- control_post: 1.25252\n",
    "- treatment_pre: 12.5042\n",
    "- treatment_post: 1.87712\n",
    "\n",
    "\n",
    "Estimate\n",
    "- did: 0.619684\n",
    "- se: 0.011303999892893988\n",
    "- p-value: 0.0\n",
    "- ci95: [0.5975278040263842, 0.6418394836065285]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
