{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39e4ac5",
   "metadata": {},
   "source": [
    "## Notebook Purpose:\n",
    "\n",
    "1. The primary goal of this notebook is to validate the results produced by the platform.\n",
    "2. We are picking one experiment = `exp_big_exp_run` and we are going to validate following:\n",
    "    - AB Results\n",
    "3. This validation will ensure if the platform is trustworthy or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0a8371",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 23:43:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Laptop_ROG_AB_validation\")\n",
    "        .master(\"spark://10.0.0.80:7077\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.executor.cores\", \"10\")\n",
    "        .config(\"spark.executor.memory\", \"18g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "\n",
    "        .config(\"spark.driver.memory\", \"10g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "        .config(\"spark.driver.host\", \"10.0.0.80\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "        # AQE + shuffle\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"288\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"256m\")\n",
    "\n",
    "        # Don’t set spark.local.dir here; use SPARK_LOCAL_DIRS on the worker\n",
    "        # .config(\"spark.local.dir\", \"...\")  <-- remove\n",
    "            # -------- MinIO / S3A (must match Iceberg) --------\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://10.0.0.80:9100\")  # <-- changed\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "        # -------- Iceberg REST catalog --------\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "        # .config(\"spark.sql.catalog.iceberg.uri\", \"http://10.0.0.59:8181\")\n",
    "        .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg-warehouse/warehouse/\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"iceberg\")\n",
    "        .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "        # Iceberg's own S3 settings (again, pointing to MinIO)\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://10.0.0.80:9100\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"minioadmin\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"minioadmin\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            \"/opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.6.0.jar,\"\n",
    "            \"/opt/spark/jars/iceberg-aws-bundle-1.6.0.jar\")\n",
    "\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----------+\n",
      "|namespace|tableName                |isTemporary|\n",
      "+---------+-------------------------+-----------+\n",
      "|exp      |analysis_results         |false      |\n",
      "|exp      |did_results              |false      |\n",
      "|exp      |experiment_params        |false      |\n",
      "|exp      |outcomes                 |false      |\n",
      "|exp      |analysis_results_cuped   |false      |\n",
      "|exp      |experiment_registry      |false      |\n",
      "|exp      |exposures                |false      |\n",
      "|exp      |metric_aggregates_daily  |false      |\n",
      "|exp      |metric_aggregates_overall|false      |\n",
      "+---------+-------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check that tables exists and we are picking experiment registry to check experiment data exists:\n",
    "spark.sql(\"Show tables in iceberg.exp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1968a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+-------------------+-------------------+-----+--------------------+---------------+-----------------+-------------------------+\n",
      "|experiment_id  |name                      |start_ts           |end_ts             |owner|variants            |primary_metrics|guardrail_metrics|notes                    |\n",
      "+---------------+--------------------------+-------------------+-------------------+-----+--------------------+---------------+-----------------+-------------------------+\n",
      "|exp_big_exp_run|Checkout button color test|2026-01-01 00:00:00|2026-01-29 00:00:00|you  |[control, treatment]|[conversion]   |[revenue]        |Synthetic demo experiment|\n",
      "+---------------+--------------------------+-------------------+-------------------+-----+--------------------+---------------+-----------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from iceberg.exp.experiment_registry where experiment_id  = 'exp_big_exp_run'\").show(truncate=False)\n",
    "\n",
    "# So the experiment does exist in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ff3146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+---------+-------------------+-------+\n",
      "|experiment_id         |user_id  |variant  |exposure_ts        |unit   |\n",
      "+----------------------+---------+---------+-------------------+-------+\n",
      "|exp_precision_long_run|u_0000000|treatment|2026-01-07 03:53:33|user_id|\n",
      "|exp_precision_long_run|u_0000002|control  |2026-01-20 19:53:12|user_id|\n",
      "|exp_precision_long_run|u_0000003|control  |2026-01-18 12:01:39|user_id|\n",
      "|exp_precision_long_run|u_0000004|treatment|2026-01-16 20:53:58|user_id|\n",
      "|exp_precision_long_run|u_0000006|control  |2026-01-18 18:29:13|user_id|\n",
      "|exp_precision_long_run|u_0000007|treatment|2026-01-16 17:38:18|user_id|\n",
      "|exp_precision_long_run|u_0000008|treatment|2026-01-03 20:18:39|user_id|\n",
      "|exp_precision_long_run|u_0000009|treatment|2026-01-06 22:58:37|user_id|\n",
      "|exp_precision_long_run|u_0000010|treatment|2026-01-19 12:22:58|user_id|\n",
      "|exp_precision_long_run|u_0000011|control  |2026-01-24 04:13:55|user_id|\n",
      "+----------------------+---------+---------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cursory view of exposures:\n",
    "\n",
    "spark.sql(\"select * from iceberg.exp.exposures limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|variant  |count(DISTINCT user_id)|\n",
      "+---------+-----------------------+\n",
      "|control  |480563                 |\n",
      "|treatment|482063                 |\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Lets validate the number of users in treatment vs control:\n",
    "\n",
    "spark.sql(\"select variant, count(distinct user_id) from iceberg.exp.exposures group by variant\").show(truncate=False)\n",
    "\n",
    "#  Looks like there: 482063 User in treatment and 480563 users in control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1152a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+------------------+-------------------+\n",
      "|experiment_id       |user_id  |metric_name|value             |ts                 |\n",
      "+--------------------+---------+-----------+------------------+-------------------+\n",
      "|exp_pricing_test_003|u_0000000|pre_revenue|19.06501624999786 |2025-12-20 02:52:10|\n",
      "|exp_pricing_test_003|u_0000001|pre_revenue|11.238445754479702|2025-12-25 13:53:17|\n",
      "|exp_pricing_test_003|u_0000002|pre_revenue|22.18493197243855 |2025-12-24 12:22:39|\n",
      "|exp_pricing_test_003|u_0000003|pre_revenue|18.2811468399141  |2025-12-27 18:20:52|\n",
      "|exp_pricing_test_003|u_0000004|pre_revenue|10.860730895283062|2025-12-30 01:01:11|\n",
      "|exp_pricing_test_003|u_0000005|pre_revenue|17.73578908053096 |2025-12-22 03:05:08|\n",
      "|exp_pricing_test_003|u_0000006|pre_revenue|14.43048618778002 |2025-12-28 20:09:49|\n",
      "|exp_pricing_test_003|u_0000007|pre_revenue|6.300105356920774 |2025-12-27 18:11:02|\n",
      "|exp_pricing_test_003|u_0000008|pre_revenue|6.9575732084637725|2025-12-23 07:38:57|\n",
      "|exp_pricing_test_003|u_0000009|pre_revenue|7.058959010267472 |2025-12-31 09:22:23|\n",
      "+--------------------+---------+-----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check the outcomes table that checks results of exposures:\n",
    "\n",
    "spark.sql(\"select * from iceberg.exp.outcomes limit 10\").show(truncate=False)\n",
    "\n",
    "#  For our validation we will focus on two metrics: conversions (primary NSM) and revenue (guardrail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4efbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|metric_name|count(DISTINCT user_id)|\n",
      "+-----------+-----------------------+\n",
      "|conversion |875355                 |\n",
      "|pre_revenue|1000000                |\n",
      "|revenue    |875355                 |\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets get a quick count:\n",
    "\n",
    "spark.sql(\"select metric_name, count(distinct user_id) from iceberg.exp.outcomes group by metric_name\").show(truncate=False)\n",
    "\n",
    "#  We are ignoring pre-revenue for now. but it becomes important for CUPED and DiD analysis\n",
    "# Looks like conversion and revenue has 875355 which is expected since we track revenue only for converted users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd82208",
   "metadata": {},
   "source": [
    "So far we have validated that there are exposures and outcomes and the sample sizes. Now we we dive into AB testing validations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d956d9",
   "metadata": {},
   "source": [
    "## AB Testing Validation:\n",
    "\n",
    "1. We will validate the summary produced by our platform i.e we get same mean, delta and relative lift by querying tables independently.\n",
    "2. Validate the results from statistical test using independent tooling such as Scipy and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c98a95",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "` Here are the results produced by the platform:`\n",
    "\n",
    "| metric | control mean | treatment mean | delta | rel lift | CI low | CI high | p-value | method |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|---|\n",
    "| conversion | 0.0999198 | 0.150077 | 0.0501574 | 0.501976 | 0.0487552 | 0.0515595 | 0 | two_proportion_z |\n",
    "| pre_revenue | 12.4993 | 12.5042 | 0.00491569 | 0.000393278 | -0.00782836 | 0.0176597 | 0.449637 | welch_normal_approx |\n",
    "| revenue | 1.25252 | 1.87712 | 0.624599 | 0.498674 | 0.606476 | 0.642723 | 0 | welch_normal_approx |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2ebdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import Libraries\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7705f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 00:40:26 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+-----+-------------------+---------+\n",
      "|experiment_id  |user_id  |metric_name|value|ts                 |variant  |\n",
      "+---------------+---------+-----------+-----+-------------------+---------+\n",
      "|exp_big_exp_run|u_0000001|conversion |0.0  |2026-01-13 14:34:21|treatment|\n",
      "|exp_big_exp_run|u_0000001|revenue    |0.0  |2026-01-13 14:34:21|treatment|\n",
      "|exp_big_exp_run|u_0000002|conversion |0.0  |2026-01-24 23:59:10|treatment|\n",
      "|exp_big_exp_run|u_0000002|revenue    |0.0  |2026-01-24 23:59:10|treatment|\n",
      "|exp_big_exp_run|u_0000003|conversion |0.0  |2026-01-14 08:00:47|control  |\n",
      "|exp_big_exp_run|u_0000003|revenue    |0.0  |2026-01-14 08:00:47|control  |\n",
      "|exp_big_exp_run|u_0000004|conversion |0.0  |2026-01-04 21:18:41|treatment|\n",
      "|exp_big_exp_run|u_0000004|revenue    |0.0  |2026-01-04 21:18:41|treatment|\n",
      "|exp_big_exp_run|u_0000005|conversion |0.0  |2026-01-20 23:02:50|control  |\n",
      "|exp_big_exp_run|u_0000005|revenue    |0.0  |2026-01-20 23:02:50|control  |\n",
      "+---------------+---------+-----------+-----+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to JOIN exposures and outcomes to measure the results\n",
    "\n",
    "v_sql = \"\"\" create or replace temporary view ab_results as\n",
    "\n",
    "select exposures.experiment_id, exposures.user_id, outcomes.metric_name, outcomes.value, outcomes.ts, exposures.variant\n",
    "\n",
    "from iceberg.exp.exposures as exposures\n",
    "inner join iceberg.exp.outcomes as outcomes\n",
    "on exposures.experiment_id = outcomes.experiment_id\n",
    "and exposures.user_id = outcomes.user_id\n",
    "and exposures.experiment_id = 'exp_big_exp_run'\n",
    "and outcomes.metric_name in ('revenue','conversion')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "spark.sql(\"select * from ab_results limit 10\").show(truncate=False)\n",
    "\n",
    "#  So the query returns data. But we need to check if there are any user_id, metric_name duplicates. Refer to next cell for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd370f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 00:40:33 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+\n",
      "|user_id|metric_name|count(1)|\n",
      "+-------+-----------+--------+\n",
      "+-------+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Please note that following SQL return blank:\n",
    "spark.sql(\"select user_id, metric_name, count(*) from ab_results group by user_id, metric_name having count(*)> 1\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a274c",
   "metadata": {},
   "source": [
    "### Validation Flow:\n",
    " For validating AB test results we need:\n",
    "\n",
    "#### Conversions: \n",
    "we will be using Two Proportions Z-Test as the metric is BINARY:\n",
    "1. We will be using statsmodels for this test: `from statsmodels.stats.proportion import proportions_ztest`\n",
    "2. Calculate total users, conversions per variant. This will give effect of the treatment, relative lift.\n",
    "3. `proportions_ztest` will give us the z and p values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb0a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 01:00:01 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+---------------+---------------------+\n",
      "|variant  |metric_name|total_users|converted_users|conversion_proportion|\n",
      "+---------+-----------+-----------+---------------+---------------------+\n",
      "|control  |conversion |424190     |42385          |0.09991984723826587  |\n",
      "|treatment|conversion |425521     |63861          |0.15007719948016668  |\n",
      "+---------+-----------+-----------+---------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 01:00:02 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\" \n",
    "select variant, metric_name, count(*) as total_users,\n",
    "count(distinct(case when value <> 0 then user_id else null end)) as converted_users,\n",
    "count(distinct(case when value <> 0 then user_id else null end))/count(*)  as conversion_proportion\n",
    "from ab_results where metric_name = 'conversion'\n",
    "group by variant, metric_name\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "\n",
    "ab_summary = spark.sql(v_sql).toPandas()\n",
    "\n",
    "ab_summary.sort_values(by = 'variant', inplace=True, ascending=False)\n",
    "\n",
    "#  Please note that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dbda53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>total_users</th>\n",
       "      <th>converted_users</th>\n",
       "      <th>conversion_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treatment</td>\n",
       "      <td>conversion</td>\n",
       "      <td>425521</td>\n",
       "      <td>63861</td>\n",
       "      <td>0.150077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>control</td>\n",
       "      <td>conversion</td>\n",
       "      <td>424190</td>\n",
       "      <td>42385</td>\n",
       "      <td>0.099920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     variant metric_name  total_users  converted_users  conversion_proportion\n",
       "1  treatment  conversion       425521            63861               0.150077\n",
       "0    control  conversion       424190            42385               0.099920"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffc45347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Series: 1    63861\n",
      "0    42385\n",
      "Name: converted_users, dtype: int64\n",
      "N Obs Series: 1    425521\n",
      "0    424190\n",
      "Name: total_users, dtype: int64\n",
      "Z-statistic: 69.8915\n",
      "P-value: 0.0000\n",
      "CI Lower Bound:  0.048755296352205955\n",
      "CI Upper Bound:  0.05155957941286664\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Control Mean</th>\n",
       "      <th>Treatment Mean</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Relative Lift</th>\n",
       "      <th>CI Lower Bound</th>\n",
       "      <th>CI Upper Bound</th>\n",
       "      <th>p-Value</th>\n",
       "      <th>z-stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09992</td>\n",
       "      <td>0.150077</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>[0.5019758699420055]</td>\n",
       "      <td>0.048755</td>\n",
       "      <td>0.05156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.891502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Control Mean  Treatment Mean     Delta         Relative Lift  \\\n",
       "0       0.09992        0.150077  0.050157  [0.5019758699420055]   \n",
       "\n",
       "   CI Lower Bound  CI Upper Bound  p-Value     z-stat  \n",
       "0        0.048755         0.05156      0.0  69.891502  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "import pandas as pd\n",
    "\n",
    "successes = ab_summary['converted_users']\n",
    "n_obs = ab_summary['total_users']\n",
    "print(f\"Success Series: {successes}\")\n",
    "print(f\"N Obs Series: {n_obs}\")\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count=successes, nobs=n_obs, alternative='two-sided')\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "lower_bound, upper_bound = confint_proportions_2indep(63861, 425521, 42385, 424190, alpha=0.05, compare='diff')\n",
    "\n",
    "print(f\"CI Lower Bound:  {lower_bound}\")\n",
    "print(f\"CI Upper Bound:  {upper_bound}\")\n",
    "\n",
    "ab_analysis_dict = {}\n",
    "ab_analysis_dict['Control Mean'] = [ab_summary[ab_summary['variant']=='control']['conversion_proportion'].values[0]]\n",
    "ab_analysis_dict['Treatment Mean'] = [ab_summary[ab_summary['variant']=='treatment']['conversion_proportion'].values[0]]\n",
    "ab_analysis_dict['Delta'] = [(ab_summary[ab_summary['variant']=='treatment']['conversion_proportion'].values[0] \n",
    "                             - ab_summary[ab_summary['variant']=='control']['conversion_proportion'].values[0])]\n",
    "\n",
    "ab_analysis_dict['Relative Lift']  = [ab_analysis_dict['Delta']/ab_summary[ab_summary['variant']=='control']['conversion_proportion'].values[0]]\n",
    "ab_analysis_dict['CI Lower Bound']  = [lower_bound]\n",
    "ab_analysis_dict['CI Upper Bound']  = [upper_bound]\n",
    "ab_analysis_dict['p-Value'] = [p_value]\n",
    "ab_analysis_dict['z-stat'] = [z_stat]\n",
    "ab_analysis_dict\n",
    "\n",
    "ab_analysis = pd.DataFrame(ab_analysis_dict)\n",
    "# ab_analysis_dict\n",
    "ab_analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406bc16",
   "metadata": {},
   "source": [
    "#### Compare above results with platform results:\n",
    "| metric | control mean | treatment mean | delta | rel lift | CI low | CI high | p-value | method |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|---|\n",
    "| conversion | 0.0999198 | 0.150077 | 0.0501574 | 0.501976 | 0.0487552 | 0.0515595 | 0 | two_proportion_z |\n",
    "\n",
    "Based on comparison we can say that conversion is a pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69592fe",
   "metadata": {},
   "source": [
    "#### Revenue Measurement:\n",
    "\n",
    "1. Since this is a continuous variable we will be using Welch's (unequal variances) test for its robustness.\n",
    "2. For this we will using SCIPY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acc1fa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 01:20:40 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|variant  |metric_name|mean_value        |\n",
      "+---------+-----------+------------------+\n",
      "|treatment|revenue    |1.8771188733014121|\n",
      "|control  |revenue    |1.2525195406211276|\n",
      "+---------+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 01:20:40 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\" \n",
    "select variant, metric_name, \n",
    "avg(value) as mean_value\n",
    "from ab_results where metric_name = 'revenue'\n",
    "group by variant, metric_name\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "\n",
    "ab_summary = spark.sql(v_sql).toPandas()\n",
    "\n",
    "ab_summary.sort_values(by = 'variant', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb3a4d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 01:22:18 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849706</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849707</th>\n",
       "      <td>control</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849708</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849709</th>\n",
       "      <td>control</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849710</th>\n",
       "      <td>control</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          variant  value\n",
       "0       treatment    0.0\n",
       "1       treatment    0.0\n",
       "2         control    0.0\n",
       "3       treatment    0.0\n",
       "4         control    0.0\n",
       "...           ...    ...\n",
       "849706  treatment    0.0\n",
       "849707    control    0.0\n",
       "849708  treatment    0.0\n",
       "849709    control    0.0\n",
       "849710    control    0.0\n",
       "\n",
       "[849711 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select variant, value\n",
    "from ab_results\n",
    "where metric_name = 'revenue'\n",
    "\n",
    "\"\"\"\n",
    "# spark.sql(v_sql).show(truncate=False)\n",
    "\n",
    "reve_data = spark.sql(v_sql).toPandas()\n",
    "reve_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "817272dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         0.0\n",
       "4         0.0\n",
       "5         0.0\n",
       "8         0.0\n",
       "11        0.0\n",
       "         ... \n",
       "849703    0.0\n",
       "849705    0.0\n",
       "849707    0.0\n",
       "849709    0.0\n",
       "849710    0.0\n",
       "Name: value, Length: 424190, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reve_data[reve_data['variant']=='control']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2bd2a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74293fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Mean: 1.2525195406211218\n",
      "Treatment Mean: 1.8771188733013897\n",
      "Delta:   0.6245993326802679\n",
      "Relative Lift:  0.49867432197547223\n",
      "Calculated T-statistic: 67.548\n",
      "P-value: 0.000\n",
      "95% Confidence Interval for the difference in means: (0.606476, 0.642723)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "control_values = np.array(reve_data[reve_data['variant']=='control']['value'])\n",
    "treatment_values = np.array(reve_data[reve_data['variant']=='treatment']['value'])\n",
    "\n",
    "\n",
    "r  = stats.ttest_ind(treatment_values, control_values, equal_var=False)\n",
    "ci = r.confidence_interval(confidence_level=0.95)\n",
    "\n",
    "print(f\"Control Mean: {np.mean(control_values)}\")\n",
    "print(f\"Treatment Mean: {np.mean(treatment_values)}\")\n",
    "print(f\"Delta:   {np.mean(treatment_values) - np.mean(control_values)}\")\n",
    "print(f\"Relative Lift:  {(np.mean(treatment_values) - np.mean(control_values))/np.mean(control_values)}\")\n",
    "print(f\"Calculated T-statistic: {r.statistic:.3f}\")\n",
    "print(f\"P-value: {r.pvalue:.3f}\")\n",
    "print(f\"95% Confidence Interval for the difference in means: ({ci.low:.6f}, {ci.high:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5810a44",
   "metadata": {},
   "source": [
    "#### Compare Above results with Platform results:\n",
    "\n",
    "| metric | control mean | treatment mean | delta | rel lift | CI low | CI high | p-value | method |\n",
    "|---|---:|---:|---:|---:|---:|---:|---:|---|\n",
    "| revenue | 1.25252 | 1.87712 | 0.624599 | 0.498674 | 0.606476 | 0.642723 | 0 | welch_normal_approx |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
