{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a84700",
   "metadata": {},
   "source": [
    "## Notebook Purpose:\n",
    "\n",
    "1. The primary goal of this notebook is to validate the results produced by the platform.\n",
    "2. We are picking one experiment = `exp_big_exp_run` and we are going to validate following:\n",
    "    - CUPED Results\n",
    "3. This validation will ensure if the platform is trustworthy or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a96ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee28245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 02:56:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Laptop_ROG_CUPED_validation\")\n",
    "        .master(\"spark://10.0.0.80:7077\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.executor.cores\", \"10\")\n",
    "        .config(\"spark.executor.memory\", \"18g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "\n",
    "        .config(\"spark.driver.memory\", \"10g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "        .config(\"spark.driver.host\", \"10.0.0.80\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "        # AQE + shuffle\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"288\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"256m\")\n",
    "\n",
    "        # Don’t set spark.local.dir here; use SPARK_LOCAL_DIRS on the worker\n",
    "        # .config(\"spark.local.dir\", \"...\")  <-- remove\n",
    "            # -------- MinIO / S3A (must match Iceberg) --------\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://10.0.0.80:9100\")  # <-- changed\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "        # -------- Iceberg REST catalog --------\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "        # .config(\"spark.sql.catalog.iceberg.uri\", \"http://10.0.0.59:8181\")\n",
    "        .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg-warehouse/warehouse/\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"iceberg\")\n",
    "        .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "        # Iceberg's own S3 settings (again, pointing to MinIO)\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://10.0.0.80:9100\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"minioadmin\")\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"minioadmin\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            \"/opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.6.0.jar,\"\n",
    "            \"/opt/spark/jars/iceberg-aws-bundle-1.6.0.jar\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129bdb9",
   "metadata": {},
   "source": [
    "## CUPED Validation:\n",
    "\n",
    "Please note that we will using pre-experiment revenue metric i.e `pre_revenue` to reduce the variance in the `revenue` metric. We are not using conversions here as CUPED works best for continuous metrics and not binary metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7c065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:06:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:06:37 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+------+------------------+\n",
      "|experiment_id  |user_id  |variant  |target|covariate         |\n",
      "+---------------+---------+---------+------+------------------+\n",
      "|exp_big_exp_run|u_0000001|treatment|0.0   |16.311533793776313|\n",
      "|exp_big_exp_run|u_0000002|treatment|0.0   |11.32790644644474 |\n",
      "|exp_big_exp_run|u_0000003|control  |0.0   |14.174728490227805|\n",
      "|exp_big_exp_run|u_0000004|treatment|0.0   |10.847600353402715|\n",
      "|exp_big_exp_run|u_0000005|control  |0.0   |8.275909435378757 |\n",
      "|exp_big_exp_run|u_0000006|control  |0.0   |12.443259526513518|\n",
      "|exp_big_exp_run|u_0000007|treatment|0.0   |15.835280615716401|\n",
      "|exp_big_exp_run|u_0000008|treatment|0.0   |7.090048635555965 |\n",
      "|exp_big_exp_run|u_0000010|control  |0.0   |9.99508481757065  |\n",
      "|exp_big_exp_run|u_0000011|treatment|0.0   |16.328538904577922|\n",
      "+---------------+---------+---------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We need to JOIN exposures and outcomes to measure the results\n",
    "\n",
    "v_sql = \"\"\" create or replace temporary view ab_results as\n",
    "with t1 as\n",
    "(select exposures.experiment_id, exposures.user_id, outcomes.metric_name, outcomes.value, outcomes.ts, exposures.variant\n",
    "\n",
    "from iceberg.exp.exposures as exposures\n",
    "inner join iceberg.exp.outcomes as outcomes\n",
    "on exposures.experiment_id = outcomes.experiment_id\n",
    "and exposures.user_id = outcomes.user_id\n",
    "and exposures.experiment_id = 'exp_big_exp_run'\n",
    "and outcomes.metric_name in ('revenue','pre_revenue'))\n",
    "\n",
    "select t1.experiment_id, t1.user_id, t1.variant,\n",
    "max(case when metric_name = 'revenue' then value else 0 end) as target,\n",
    "max(case when metric_name = 'pre_revenue' then value else 0 end) as covariate\n",
    "from t1\n",
    "group by  t1.experiment_id, t1.user_id, t1.variant\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "spark.sql(\"select * from ab_results limit 10\").show(truncate=False)\n",
    "\n",
    "#  So the query returns data. But we need to check if there are any user_id, metric_name duplicates. Refer to next cell for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed623f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:10:05 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select avg(covariate) as mean_covariate,\n",
    "var_samp(covariate) as var_covariate,\n",
    "covar_samp( target,covariate) as covar\n",
    "from ab_results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cuped_vals = spark.sql(v_sql).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad792f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5017229475785\n",
      "8.980809420322341\n",
      "0.007240231252164588\n",
      "0.0008061891655089503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_covariate = cuped_vals['mean_covariate'].values[0]\n",
    "var_covariate = cuped_vals['var_covariate'].values[0]\n",
    "covar = cuped_vals['covar'].values[0]\n",
    "theta = covar/var_covariate\n",
    "\n",
    "print(mean_covariate)\n",
    "print(var_covariate)\n",
    "print(covar)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Please note that for cuped need adjusted target y-value based on reduced variance:\n",
    "# We are calculating this at individual user level instead of group level\n",
    "# Adjusted Y = Y - theta*( pre_experiment_value - mean of pre_experiment_value)\n",
    "# We will re-run earlier join and inject values from above there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89704283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:23:40 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+------+------------------+----------------------+\n",
      "|experiment_id  |user_id  |variant  |target|covariate         |adjusted_target       |\n",
      "+---------------+---------+---------+------+------------------+----------------------+\n",
      "|exp_big_exp_run|u_0000001|treatment|0.0   |16.311533793776313|-0.0030714282268431625|\n",
      "|exp_big_exp_run|u_0000002|treatment|0.0   |11.32790644644474 |9.463181455096624E-4  |\n",
      "|exp_big_exp_run|u_0000003|control  |0.0   |14.174728490227805|-0.0013487589423202917|\n",
      "|exp_big_exp_run|u_0000004|treatment|0.0   |10.847600353402715|0.0013335357138480762 |\n",
      "|exp_big_exp_run|u_0000005|control  |0.0   |8.275909435378757 |0.003406805068996757  |\n",
      "|exp_big_exp_run|u_0000006|control  |0.0   |12.443259526513518|4.713257664117599E-5  |\n",
      "|exp_big_exp_run|u_0000007|treatment|0.0   |15.835280615716401|-0.002687478074652057 |\n",
      "|exp_big_exp_run|u_0000008|treatment|0.0   |7.090048635555965 |0.00436283319761567   |\n",
      "|exp_big_exp_run|u_0000010|control  |0.0   |9.99508481757065  |0.0020208245022639445 |\n",
      "|exp_big_exp_run|u_0000011|treatment|0.0   |16.328538904577922|-0.003085137562929699 |\n",
      "+---------------+---------+---------+------+------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We need to JOIN exposures and outcomes to measure the results\n",
    "\n",
    "v_sql = f\"\"\" create or replace temporary view ab_results_2 as\n",
    "with t1 as\n",
    "(select exposures.experiment_id, exposures.user_id, outcomes.metric_name, outcomes.value, outcomes.ts, exposures.variant\n",
    "\n",
    "from iceberg.exp.exposures as exposures\n",
    "inner join iceberg.exp.outcomes as outcomes\n",
    "on exposures.experiment_id = outcomes.experiment_id\n",
    "and exposures.user_id = outcomes.user_id\n",
    "and exposures.experiment_id = 'exp_big_exp_run'\n",
    "and outcomes.metric_name in ('revenue','pre_revenue')),\n",
    "\n",
    "t2 as\n",
    "(\n",
    "select t1.experiment_id, t1.user_id, t1.variant,\n",
    "max(case when metric_name = 'revenue' then value else 0 end) as target,\n",
    "max(case when metric_name = 'pre_revenue' then value else 0 end) as covariate\n",
    "from t1\n",
    "group by  t1.experiment_id, t1.user_id, t1.variant)\n",
    "\n",
    "select t2.*, (target - {theta}*(covariate - {mean_covariate})) as adjusted_target\n",
    "\n",
    "from t2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "spark.sql(\"select * from ab_results_2 limit 10\").show(truncate=False)\n",
    "\n",
    "#  So the query returns data. But we need to check if there are any user_id, metric_name duplicates. Refer to next cell for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89e33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:26:22 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "26/02/19 03:26:24 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|variant  |mean_value        |\n",
      "+---------+------------------+\n",
      "|treatment|1.8771168949176746|\n",
      "|control  |1.252521525212478 |\n",
      "+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# After we have calculated adjusted target values we run the normal AB testing formulas:\n",
    "\n",
    "v_sql = \"\"\" \n",
    "select variant, \n",
    "avg(adjusted_target) as mean_value\n",
    "from ab_results_2 \n",
    "\n",
    "group by variant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(v_sql).show(truncate=False)\n",
    "\n",
    "ab_summary = spark.sql(v_sql).toPandas()\n",
    "\n",
    "ab_summary.sort_values(by = 'variant', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "403f9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/19 03:27:30 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>treatment</td>\n",
       "      <td>-0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>-0.001349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.001334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>0.003407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849706</th>\n",
       "      <td>treatment</td>\n",
       "      <td>-0.002597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849707</th>\n",
       "      <td>control</td>\n",
       "      <td>-0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849708</th>\n",
       "      <td>treatment</td>\n",
       "      <td>0.001407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849709</th>\n",
       "      <td>control</td>\n",
       "      <td>-0.001092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849710</th>\n",
       "      <td>control</td>\n",
       "      <td>-0.001711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          variant     value\n",
       "0       treatment -0.003071\n",
       "1       treatment  0.000946\n",
       "2         control -0.001349\n",
       "3       treatment  0.001334\n",
       "4         control  0.003407\n",
       "...           ...       ...\n",
       "849706  treatment -0.002597\n",
       "849707    control -0.000160\n",
       "849708  treatment  0.001407\n",
       "849709    control -0.001092\n",
       "849710    control -0.001711\n",
       "\n",
       "[849711 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sql = \"\"\"\n",
    "select variant, adjusted_target as value\n",
    "from ab_results_2\n",
    "\n",
    "\"\"\"\n",
    "# spark.sql(v_sql).show(truncate=False)\n",
    "\n",
    "reve_data = spark.sql(v_sql).toPandas()\n",
    "reve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc1aa60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: 0.0008061891655089503\n",
      "x-Mean: 12.5017229475785\n",
      "Control Mean: 1.252521525212505\n",
      "Treatment Mean: 1.8771168949176709\n",
      "Delta:   0.6245953697051658\n",
      "Relative Lift:  0.4986703678399426\n",
      "Calculated T-statistic: 67.547\n",
      "P-value: 0.000\n",
      "95% Confidence Interval for the difference in means: (0.606472, 0.642719)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "control_values = np.array(reve_data[reve_data['variant']=='control']['value'])\n",
    "treatment_values = np.array(reve_data[reve_data['variant']=='treatment']['value'])\n",
    "\n",
    "\n",
    "r  = stats.ttest_ind(treatment_values, control_values, equal_var=False)\n",
    "ci = r.confidence_interval(confidence_level=0.95)\n",
    "\n",
    "print(f\"Theta: {theta}\")\n",
    "print(f\"x-Mean: {mean_covariate}\")\n",
    "print(f\"Control Mean: {np.mean(control_values)}\")\n",
    "print(f\"Treatment Mean: {np.mean(treatment_values)}\")\n",
    "print(f\"Delta:   {np.mean(treatment_values) - np.mean(control_values)}\")\n",
    "print(f\"Relative Lift:  {(np.mean(treatment_values) - np.mean(control_values))/np.mean(control_values)}\")\n",
    "print(f\"Calculated T-statistic: {r.statistic:.3f}\")\n",
    "print(f\"P-value: {r.pvalue:.3f}\")\n",
    "print(f\"95% Confidence Interval for the difference in means: ({ci.low:.6f}, {ci.high:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d67789",
   "metadata": {},
   "source": [
    "## Comparison between Manual vs Platform values:\n",
    "\n",
    "**Conclusion:** Manually calculated values are same as platform and hence CUPED set up in platform works as intended.\n",
    "\n",
    "**Manual Values:**\n",
    "\n",
    "```\n",
    "Theta: 0.0008061891655089503\n",
    "x-Mean: 12.5017229475785\n",
    "Control Mean: 1.252521525212505\n",
    "Treatment Mean: 1.8771168949176709\n",
    "Delta:   0.6245953697051658\n",
    "Relative Lift:  0.4986703678399426\n",
    "Calculated T-statistic: 67.547\n",
    "P-value: 0.000\n",
    "95% Confidence Interval for the difference in means: (0.606472, 0.642719)\n",
    "```\n",
    "\n",
    "**Platform Values:**\n",
    "\n",
    "```\n",
    "Target metric: `revenue`\n",
    "Covariate: `pre_revenue`\n",
    "\n",
    "- theta: `0.0008061891655`\n",
    "- x_mean: `12.5017`\n",
    "\n",
    "### Raw (no CUPED)\n",
    "- control mean: `1.25252`\n",
    "- treatment mean: `1.87712`\n",
    "- delta: `0.624599`\n",
    "- p-value: `0.0`\n",
    "- ci95: `[0.6064755764100439, 0.6427230889505252]`\n",
    "\n",
    "### CUPED-adjusted\n",
    "- control mean: `1.25252`\n",
    "- treatment mean: `1.87712`\n",
    "- delta: `0.624595`\n",
    "- p-value: `0.0`  (not stored in Iceberg schema)\n",
    "- ci95: `[0.6064716157553894, 0.6427191236550062]\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
